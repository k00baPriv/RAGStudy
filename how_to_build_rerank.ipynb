{
 "cells": [
  {
   "cell_type": "code",
   "id": "eb2bfac85fe38492",
   "metadata": {},
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# Query and top-k documents\n",
    "query = \"machine learning techniques\""
   ],
   "id": "39b9292a598d0699"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "documents = [\n",
    "    \"Deep learning and its applications\",\n",
    "    \"Introduction to machine learning\",\n",
    "    \"Statistical methods in AI\",\n",
    "]"
   ],
   "id": "e1c3557de0bf40fd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Let's break down the TF-IDF calculation for the sentence **\"Deep learning and its applications\"** step by step. We'll calculate the **TF-IDF score** for each term in the sentence based on a small, hypothetical corpus for simplicity.\n",
    "\n",
    "---\n",
    "\n",
    "### Corpus (Documents)\n",
    "We'll use the following corpus:\n",
    "1. Document 1: \"Deep learning and its applications\"\n",
    "2. Document 2: \"Deep learning is a subset of machine learning\"\n",
    "3. Document 3: \"Applications of machine learning include image recognition\"\n",
    "\n",
    "---\n",
    "\n",
    "### Step 1: Tokenization\n",
    "First, split each document into individual terms (words). Ignore case and remove punctuation.\n",
    "\n",
    "**Document 1**: [\"deep\", \"learning\", \"and\", \"its\", \"applications\"]  \n",
    "**Document 2**: [\"deep\", \"learning\", \"is\", \"a\", \"subset\", \"of\", \"machine\", \"learning\"]  \n",
    "**Document 3**: [\"applications\", \"of\", \"machine\", \"learning\", \"include\", \"image\", \"recognition\"]\n",
    "\n",
    "---\n",
    "\n",
    "### Step 2: Calculate Term Frequency (TF)\n",
    "The **term frequency (TF)** measures how often a word appears in a document relative to the total number of words in that document.\n",
    "\n",
    "$$\n",
    "\\text{TF}(t, d) = \\frac{\\text{Frequency of } t \\text{ in } d}{\\text{Total terms in } d}\n",
    "$$\n",
    "\n",
    "For Document 1 (**\"Deep learning and its applications\"**):\n",
    "\n",
    "| Term           | Frequency | Total Terms | TF (Term Frequency)        |\n",
    "|----------------|-----------|-------------|-----------------------------|\n",
    "| deep           | 1         | 5           | $ \\frac{1}{5} = 0.2 $    |\n",
    "| learning       | 1         | 5           | $ \\frac{1}{5} = 0.2 $    |\n",
    "| and            | 1         | 5           | $ \\frac{1}{5} = 0.2 $    |\n",
    "| its            | 1         | 5           | $ \\frac{1}{5} = 0.2 $    |\n",
    "| applications   | 1         | 5           | $ \\frac{1}{5} = 0.2 $    |\n",
    "\n",
    "---\n",
    "\n",
    "### Step 3: Calculate Inverse Document Frequency (IDF)\n",
    "The **inverse document frequency (IDF)** measures how important a term is across the entire corpus. Rare terms have higher IDF values, while common terms have lower values.\n",
    "\n",
    "$$\n",
    "\\text{IDF}(t) = \\log \\left( \\frac{N}{\\text{DF}(t)} \\right)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ N $ is the total number of documents.\n",
    "- $ \\text{DF}(t) $ is the number of documents containing the term $ t $.\n",
    "\n",
    "| Term           | DF (Number of Documents with Term) | Total Documents (N) | IDF                                      |\n",
    "|----------------|------------------------------------|----------------------|------------------------------------------|\n",
    "| deep           | 2                                  | 3                    | $ \\log \\left( \\frac{3}{2} \\right) = 0.176 $ |\n",
    "| learning       | 3                                  | 3                    | $ \\log \\left( \\frac{3}{3} \\right) = 0.0 $     |\n",
    "| and            | 1                                  | 3                    | $ \\log \\left( \\frac{3}{1} \\right) = 0.477 $ |\n",
    "| its            | 1                                  | 3                    | $ \\log \\left( \\frac{3}{1} \\right) = 0.477 $ |\n",
    "| applications   | 2                                  | 3                    | $ \\log \\left( \\frac{3}{2} \\right) = 0.176 $ |\n",
    "\n",
    "---\n",
    "\n",
    "### Step 4: Calculate TF-IDF\n",
    "Finally, compute the **TF-IDF** score for each term by multiplying the term frequency (TF) by the inverse document frequency (IDF):\n",
    "\n",
    "$$\n",
    "\\text{TF-IDF}(t, d) = \\text{TF}(t, d) \\times \\text{IDF}(t)\n",
    "$$\n",
    "\n",
    "For Document 1 (**\"Deep learning and its applications\"**):\n",
    "\n",
    "| Term           | TF     | IDF       | TF-IDF                |\n",
    "|----------------|--------|-----------|-----------------------|\n",
    "| deep           | 0.2    | 0.176     | $ 0.2 \\times 0.176 = 0.0352 $  |\n",
    "| learning       | 0.2    | 0.0       | $ 0.2 \\times 0.0 = 0.0 $       |\n",
    "| and            | 0.2    | 0.477     | $ 0.2 \\times 0.477 = 0.0954 $  |\n",
    "| its            | 0.2    | 0.477     | $ 0.2 \\times 0.477 = 0.0954 $  |\n",
    "| applications   | 0.2    | 0.176     | $ 0.2 \\times 0.176 = 0.0352 $  |\n",
    "\n",
    "---\n",
    "\n",
    "### Final TF-IDF Vector for Document 1\n",
    "The vector representation of the document \"Deep learning and its applications\" based on TF-IDF is:\n",
    "\n",
    "$$\n",
    "\\mathbf{v} = [0.0352, 0.0, 0.0954, 0.0954, 0.0352]\n",
    "$$\n",
    "\n",
    "This vector can now be used to compare the document with others (or a query) using similarity measures like **cosine similarity**."
   ],
   "id": "fcad2cd1bf8765ec"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We use the **logarithmic measure** in the **Inverse Document Frequency (IDF)** calculation to control the impact of term frequency across documents. The logarithm ensures that the IDF values grow more moderately as a term becomes rarer and prevents the measure from disproportionately favoring extremely rare terms. Here’s a detailed explanation:\n",
    "\n",
    "---\n",
    "\n",
    "### 1. **Handling Growth in Document Frequency**\n",
    "The formula for IDF without a logarithm is:\n",
    "\n",
    "$$\n",
    "\\text{IDF}(t) = \\frac{N}{\\text{DF}(t)}\n",
    "$$\n",
    "\n",
    "Here:\n",
    "- $ N $ is the total number of documents.\n",
    "- $ \\text{DF}(t) $ is the number of documents containing the term $ t $.\n",
    "\n",
    "Without the logarithm, the IDF value grows linearly as $ \\text{DF}(t) $ decreases. For very rare terms (e.g., terms that appear in only 1 out of 1,000,000 documents), the IDF value can become extremely large, which can dominate the overall TF-IDF calculation and skew the importance of terms disproportionately.\n",
    "\n",
    "By taking the logarithm, we **dampen this growth**, ensuring that IDF increases at a slower rate for rare terms. This makes the measure more stable and interpretable.\n",
    "\n",
    "$$\n",
    "\\text{IDF}(t) = \\log \\left( \\frac{N}{\\text{DF}(t)} \\right)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Emphasizing Rare Terms (But Not Too Much)**\n",
    "The purpose of IDF is to assign higher importance to rare terms, as they are more likely to be meaningful or unique to specific documents. For instance:\n",
    "- A term appearing in only 1 document out of 1,000,000 is clearly rare and important.\n",
    "- A term appearing in 900,000 documents is common and less informative.\n",
    "\n",
    "Using a logarithm ensures that **rare terms get higher scores**, but it prevents the scores from becoming excessively large. For example:\n",
    "- Without logarithm: $ \\text{IDF}(t) = \\frac{10,000}{1} = 10,000 $ for a rare term.\n",
    "- With logarithm: $ \\text{IDF}(t) = \\log(10,000) = 4 $.\n",
    "\n",
    "This moderated score keeps the TF-IDF values reasonable.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Smoothing the Scale**\n",
    "Logarithms compress large ranges into smaller, manageable scales, which is particularly helpful in text data where document frequencies can vary wildly. For example:\n",
    "- A term appearing in 1 document has an IDF of $ \\log(\\frac{1000}{1}) = 3 $.\n",
    "- A term appearing in 10 documents has an IDF of $ \\log(\\frac{1000}{10}) = 2 $.\n",
    "- A term appearing in 100 documents has an IDF of $ \\log(\\frac{1000}{100}) = 1 $.\n",
    "\n",
    "This smooth progression ensures that the impact of IDF is proportional, not overly biased toward extreme rarity.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Prevents Overweighting Noise**\n",
    "Extremely rare terms could sometimes be noise or typographical errors. Without logarithmic scaling, their IDF values would dominate the TF-IDF calculation. The logarithm dampens their influence, making the calculation more robust to noisy data.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **Aligns with Information Theory**\n",
    "The logarithm in IDF aligns with the principles of **information theory**, where the informativeness of an event is inversely related to its probability. The rarer a term is (lower $ \\text{DF}(t) $), the more \"informative\" it is, and the logarithmic function models this relationship effectively.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary\n",
    "Using a logarithmic measure in IDF ensures:\n",
    "1. Rare terms are emphasized but not overly dominant.\n",
    "2. The scale of IDF values remains interpretable and manageable.\n",
    "3. The calculation is more robust to noise and extreme values.\n",
    "4. The measure aligns with the principles of information theory, reflecting term importance in a meaningful way."
   ],
   "id": "971aa4069fee7cf2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Step 1: TF-IDF vectorization\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform([query] + documents)"
   ],
   "id": "7d8ee6ff6d9dc37"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The goal is to represent the query and documents in the same vector space, allowing us to compute similarities between them. Using TF-IDF ensures that:\n",
    "\n",
    "Common terms (e.g., \"the\", \"and\") are downweighted because they have low importance (low IDF).\n",
    "Rare terms (e.g., \"machine\", \"learning\") are given higher importance if they are significant to the query or document.\n",
    "This representation is then used to calculate cosine similarity between the query and the documents for reranking or relevance scoring."
   ],
   "id": "beb0c318138b7a5d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T19:00:23.119400Z",
     "start_time": "2025-01-12T19:00:23.114114Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Convert the sparse matrix to a dense format\n",
    "dense_matrix = tfidf_matrix.toarray()\n",
    "\n",
    "# Get feature names (terms in the vocabulary)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Print the TF-IDF matrix with terms\n",
    "print(\"TF-IDF Matrix:\")\n",
    "print(f\"Terms: {feature_names}\")\n",
    "print(dense_matrix)"
   ],
   "id": "e7bedc2c3afe53ea",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Matrix:\n",
      "Terms: ['ai' 'and' 'applications' 'deep' 'in' 'introduction' 'its' 'learning'\n",
      " 'machine' 'methods' 'statistical' 'techniques' 'to']\n",
      "[[0.         0.         0.         0.         0.         0.\n",
      "  0.         0.44809973 0.55349232 0.         0.         0.70203482\n",
      "  0.        ]\n",
      " [0.         0.47633035 0.47633035 0.47633035 0.         0.\n",
      "  0.47633035 0.30403549 0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.         0.57457953\n",
      "  0.         0.36674667 0.4530051  0.         0.         0.\n",
      "  0.57457953]\n",
      " [0.5        0.         0.         0.         0.5        0.\n",
      "  0.         0.         0.         0.5        0.5        0.\n",
      "  0.        ]]\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# Step 2: Compute cosine similarity\n",
    "similarity_scores = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:]).flatten()"
   ],
   "id": "def13b8d7e736e06"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "In this step, the idea is to calculate the similarity between the query and the documents, and while it might seem like they need to share the exact same words to match, that’s not entirely true. The goal here is to use **TF-IDF and cosine similarity** to find how relevant each document is to the query, and this goes beyond simple word matching.\n",
    "\n",
    "Yes, if the query and the document share the same words, it definitely helps. For example, if the query is \"machine learning techniques\" and the document says \"Introduction to machine learning,\" the shared words like \"machine\" and \"learning\" contribute to a higher cosine similarity. This happens because those words get higher **TF-IDF weights**, and their alignment increases the similarity score.\n",
    "\n",
    "But the key here is that TF-IDF doesn’t treat all words equally. It focuses on **important terms**, meaning words that are rare across the entire corpus but frequent in a specific document or query. For instance, a term like \"techniques\" will carry more weight than a common word like \"and\" or \"is.\" So, even if the query and document share a lot of common but unimportant words, their similarity might still be low if they miss the key terms.\n",
    "\n",
    "One limitation, though, is that this approach doesn’t inherently handle synonyms or related words. For example, if my query is \"methods for machine learning\" and the document says \"techniques in machine learning,\" the cosine similarity might not be as high as expected because the words \"methods\" and \"techniques\" don’t match exactly. This is where TF-IDF falls short compared to models like embeddings, which capture deeper semantic relationships.\n",
    "\n",
    "So, the purpose of this code is to measure how well the terms in the query align with those in the documents, weighted by their importance. It works great for tasks where exact or near-exact word matching is sufficient. But for cases where I need to account for synonyms or broader contextual relevance, I might need to combine this with more advanced approaches like dense embeddings."
   ],
   "id": "f54ef35dd6ced442"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# Step 3: Rerank documents based on similarity scores\n",
    "reranked_indices = similarity_scores.argsort()[::-1]\n",
    "reranked_documents = [documents[i] for i in reranked_indices]"
   ],
   "id": "6d7998bd4c82b49"
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-12T19:08:03.217609Z",
     "start_time": "2025-01-12T19:08:03.212378Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Reranked Documents:\")\n",
    "for i, doc in enumerate(reranked_documents):\n",
    "    print(f\"Rank {i+1}: {doc}\")\n"
   ],
   "id": "initial_id",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reranked Documents:\n",
      "Rank 1: Introduction to machine learning\n",
      "Rank 2: Deep learning and its applications\n",
      "Rank 3: Statistical methods in AI\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "e68942f85e21d611"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
